% Chapter 3: Theory Development
\chapter{Theory Development}
\label{chapter:theory-development}

\graphicspath{ {report/C3 Theory Development/assets/} } 

\section{Theoretical Development of SSVEP Decoding Algorithms}

\subsection{Canonical correlation analysis (CCA)}
\label{section:cca-c3}
Given variable partitions $\mX \in \R^{m\times p}$, $\mY \in \R^{m\times q}$ each with $m$ observations, CCA seeks to find a linear combination of $\mX$, $\mY$ that maximises the correlation between their images. More formally, assuming that the variables (columns) of the two partitions $\vx_1, \dots, \vx_p \in \R^{m}$ and $\vy_1, \dots, \vy_q \in \R^{m}$ are standardised to have zero mean and unit variance, find
\begin{align}
    \rho &= \max_{\vw_X, \vw_Y} \textrm{corr}(\mX \vw_X, \mY \vw_Y) \\
    &= \max_{\vw_X, \vw_Y} \frac{\E[\mX \vw_X\vw_Y^\top\mY^\top ]}{\sqrt{\E[\mX \vw_X\vw_X^\top\mX^\top]\E[\mY \vw_Y\vw_Y^\top\mY^\top]}}
    \label{eq:cca-corr-objective}
\end{align}
where $\rho$ is the \textit{canonical correlation}, $\vw_X$ and $\vw_Y$ are the canonical weight vectors and the images $\vz_X = \mX\vw_X$ and $\vz_Y = \mY\vw_Y$ are the \textit{canonical variables} for $\mX$ and $\mY$ respectively.

\subsubsection{Geometric interpretation}
The authors in \cite{cca-tutorial} present an intuitive geometric analogy. Consider $\mX$ and $\mY$ as linear transforms of position vectors $\vw_X$ and $\vw_Y$ onto their images $\vz_X$ and $\vz_Y$ in $\R^m$. CCA constrains $\vz_X$ and $\vz_Y$ to have unit norm and that the angle $\theta \in [0, \frac{\pi}{2}]$ between them be minimised so as to maximise their correlation. The canonical correlation $\rho$ is then the inner product of $\vz_X$ and $\vz_Y$:
\begin{equation}
    \rho = \vz_X^\top\vz_Y = \|\vz_X\| \|\vz_Y\|\cos(\theta) = \cos(\theta)
\end{equation}
Therefore, CCA aims to find position vectors $\vw_X$, $\vw_Y$ that, after undergoing linear transforms $\mX$ and $\mY$, are mapped to a unit sphere in $\R^m$ where $\theta$ is minimised \cite{cca-tutorial}. Note that there will be $n=\textrm{min}(p, q)$ canonical correlations $\rho_1, \dots, \rho_n$ with $|\rho_i| \geq |\rho_j|, \; \forall \; i < j$. Typically, only the first and largest canonical correlation $\rho_1$ is considered. 

\subsubsection{Solving for canonical weight vectors through the eigenvalue problem}
Consider the case of solving for the first and largest canonical correlation $p_1$. Following the geometric interpretation, the objective is to find
\begin{equation}
    \cos(\theta_1) = \max_{\vw_X, \vw_Y}\langle\,\vz_X,\vz_Y\rangle \quad \textrm{with} \quad \|\vz_X\| = \|\vz_Y\| = 1
\label{eq:cca-objective-1}
\end{equation}
where $\theta_1$ is the smallest angle between image vectors. Defining the within-set covariance matrices as 
\begin{align}
    \mC_{XX} = \mX^\top\mX \quad \textrm{and} \quad
    \mC_{YY} = \mY^\top \mY 
\end{align}
and the inter-set cross covariance matrix as 
\begin{align}
    \mC_{XY} &= \mX^\top\mY,
\end{align}
the unit variance constraint on the image vectors can be rewritten as  \begin{align}
\begin{split}
    \|\vz_X\| &= \vz_X^\top\vz_X = \vw_X^\top \mX^\top \mX \vw_X =  \vw_X^\top \mC_{XX} \vw_X = 1 \\
    \|\vz_Y\| &= \vz_Y^\top\vz_Y = \vw_Y^\top \mY^\top \mY \vw_Y =  \vw_Y^\top \mC_{YY} \vw_Y = 1 
    \end{split}
\label{eq:unit-variance-reformulated}
\end{align}
Substituting (\ref{eq:unit-variance-reformulated}) into (\ref{eq:cca-corr-objective}) yields 

\begin{equation}
\cos(\theta) = \max_{\vw_X, \vw_Y} \vw_X^\top \mC_{XY} \vw_Y 
\end{equation}
Lagrangian dual theory provides a straight forward way to solve the constrained optimisation problem above with unit variance constraints as in (\ref{eq:unit-variance-reformulated}). Consider the Lagrangian for this problem below with Lagrange multipliers $\lambda_1$ and $\lambda_2$:
\begin{equation}
    \mathcal{L}(\vw_X, \vw_Y, \lambda_1, \lambda_2) = \vw_X^\top \mC_{XY} \vw_Y - \frac{\lambda_1}{2}(\vw_X^\top \mC_{XX} \vw_X-1) - \frac{\lambda_2}{2}(\vw_Y^\top \mC_{YY} \vw_Y-1)
\end{equation}
With reference to the KKT conditions, stationary requires
\begin{align}
    \label{eq:cca-lagrange-x}
    \frac{\partial \mathcal{L}}{\partial \vw_X} &= \mC_{XY} \vw_Y - \lambda_1\mC_{XX} \vw_X = 0 \\
    \frac{\partial \mathcal{L}}{\partial \vw_Y} &= \mC_{YX} \vw_X - \lambda_2\mC_{YY} \vw_Y = 0
        \label{eq:cca-lagrange-y}
\end{align}
Since $\mC_{YX} = \mC_{XY}^\top$. Pre-multiplying (\ref{eq:cca-lagrange-x}) by $\vw_X^\top$ and (\ref{eq:cca-lagrange-y}) by $\vw_Y^\top$ yields:
\begin{align}
\begin{split}
    \vw_X^\top\mC_{XY} \vw_Y - \lambda_1\vw_X^\top\mC_{XX} \vw_X &= 0 \\
    \vw_Y^\top\mC_{YX} \vw_X - \lambda_2\vw_Y^\top\mC_{YY} \vw_Y &= 0
\end{split}
\label{eq:lagrange-multi}
\end{align}
Since $\vw_X^\top \mC_{XX} \vw_X = \vw_Y^\top \mC_{YY} \vw_Y =1$, it is clear from (\ref{eq:lagrange-multi}) that $\lambda_1=\lambda_2 = \lambda$. Substituting this result into (\ref{eq:cca-lagrange-x}) yields
\begin{align}
    \vw_X = \frac{\mC_{XX}^{-1}C_{XY}\vw_Y}{\lambda}
\end{align}
Then, substituting into (\ref{eq:cca-lagrange-y}) produces the following generalised eigenvalue problem:
\begin{align}
    \mC_{YX}\mC^{-1}_{XX}\mC_{XY}\vw_{Y} = \lambda^2\mC_{YY}\vw_Y
\end{align}
If $\mC_{YY}$ is non-singular, this reduces to a standard eigenvalue problem of the form
\begin{align}
    \mC_{YY}^{-1}\mC_{YX}\mC^{-1}_{XX}\mC_{XY}\vw_{Y} = \lambda^2\vw_Y
\end{align}
The eigenvalues of the matrix $\mC_{YY}^{-1}\mC_{YX}\mC^{-1}_{XX}\mC_{XY}$ correspond to the squares of the canonical correlations $\rho_1, \dots, \rho_n$.

\subsection{Task-related component analysis (TRCA)}
\label{sec:trca-c3}
Expanding from the toy example in Section \ref{section:trca-c2}, the TRCA algorithm implicitly assumes that observed signals are generated as a linear combination of task-related and task-unrelated components which implies that task-related components can be recovered by applying an appropriate weighting of observed signals across trials \cite{tanaka-trca}. Inter-trial covariance maximisation is the core objective that aims to extract task-related components with maximal temporal similarity across trials. 

Now that multiple trials are to be considered, the signal matrix $\mX\in \R^{N_s \times N_c}$ introduced in Section \ref{section:cca-c3} must be extended to a third order signal tensor $\tX \in \R^{N_s \times N_c \times N_t}$ with $N_t$ trials, each with $N_c$ channels\footnote{the constraint that the number of variables (width of $\mX_i$) for each trial be constant is not strictly necessary. For this application, however, it is assumed that the number of channels will not change between trials of a given experiment.} and $N_s$ samples. Let $\vy^{(k)}$ be the $k$-th trial (block) with $k\in\{1, 2, \dots, N_t\}$ of the signal $y(t)$ and $\vx_i^{(k)}$ be the $k$-th trial of the input signal from channel $i, \, i\in\{1, \dots, N_c\}$. Consider the covariance between all $i$ channels for two trials $k$ and $l$:

\begin{equation}
    \widehat{\mC}_{k l}=\operatorname{Cov}\left(\vy^{(k)}, \vy^{(l)}\right)=\sum_{i, \, j=1}^{N_c} w_{i} w_{j} \operatorname{Cov}\left(\vx_{i}^{(k)}, \vx_{k}^{(l)}\right)
\end{equation}
Furthermore, as with CCA, we must impose the constraint on the designed weights such that the variance of $y(t)$ is bounded:
\begin{equation}
    \operatorname{Var}(\vy)=\sum_{i, \, j=1}^{N_c} w_{i} w_{j} \operatorname{Cov}\left(\vx_{i}, \vx_{j}\right)=\mathbf{w}^{\mathrm{T}} \mQ \mathbf{w}=1
\end{equation}
Then, all possible combinations of trials $1, \dots, N_t$ are summed as
\begin{align}
\begin{split}
\sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \widehat{\mC}_{k l} &=\sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \operatorname{Cov}\left(\vy^{(k)}, \vy^{(l)}\right) \\
& =\sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \sum_{i, j=1}^{N_c} w_{i} w_{j} \operatorname{Cov}\left(\vx_{i}^{(k)}, \vx_{j}^{(l)}\right)= \vw^\top \mS \vw
\end{split}
\label{eq:trca-cov-sum}
\end{align}
where $\mS$ is the following symmetric matrix
\begin{equation}
    \mS_{i, j} = \sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \operatorname{Cov}\left(\vx_{i}^{(k)}, \vx_{j}^{(l)}\right)
\end{equation}
As alluded to before, (\ref{eq:trca-cov-sum}) yields the sum of covariances of all combinations of trials: a measure of task consistency. This is precisely what we want to maximise. This constrained optimisation problem (recalling the variance constraint on $y(t)$) can be reformulated as a Rayleigh-Ritz eigenvalue problem of the form 

\begin{equation}
    \vw^*=\underset{\vw}{\arg \max } \frac{\vw^\top \mS \vw}{\vw^\top \mQ \vw}
\label{eq:trca-rayleigh-ritz}
\end{equation}
The inter-trial weight vectors $\vw_k, \, k \in \{1, \dots, N_t\}$ can be found by computing the eigenvectors of $\mQ^{-1}\mS$. Specifically, the degree of task-relatedness corresponds to the magnitude of the eigenvalues of $\mQ^{-1}\mS$ and so the optimal weight $\vw^*$ corresponds to the largest eigenvalue.

\subsection{Multiset CCA (MsetCCA)}
MsetCCA is an extension of standard CCA to multiple data sets or partitions. Accordingly, the objective is now to maximise the correlation between canonical variables from \textit{many sets} of observations at a given stimulus frequency $f_k$. Although several objective functions for MsetCCA exist, the MAXVAR objective is explored in \cite{zhang-mset-cca} for its intuitive extension to ordinary CCA with multiple variable sets. Assuming all sets of variables in $\tX$ are normalised to have zero mean and unit variance, the MAXVAR objective for maximising correlation over canonical variables from multiple sets at a given candidate frequency $f_k$ is defined as follows:
\begin{align}
\begin{split}
\max_{\mathbf{w}_{1}, \ldots, \mathbf{w}_{N_t}} & \quad \rho=\sum_{i \neq j}^{N_t} \mathbf{w}_{i}^\top\mX_{i}^\top \mX_{j}\mathbf{w}_{j} \\
\text { s.t. } & \quad \frac{1}{N_t} \sum_{i=1}^{N_t} \mathbf{w}_{i}^\top\mX_{i}^\top \mX_{i}\mathbf{w}_{i}=1,
\end{split}
\label{eq:mset-cca-objective}
\end{align}
where the same conventions for the signal tensor $\tX$ as in Section \ref{sec:trca-c3} above apply. Following a similar approach as in the derivation for CCA above, the method of Lagrange multipliers can be used to transform the constrained optimisation problem in (\ref{eq:mset-cca-objective}) to a generalised eigenvalue problem of the form:
\begin{equation}
        (\mR -\mS)\vw = \rho\mS \vw
\label{eq:mset-cca-gen-eig}
\end{equation}
where
\begin{align*}
\mR = \begin{bmatrix}
\mX_{1}^{\top}\mX_{1} & \ldots & \mX_{1}^{\top}\mX_{N_t} \\
\vdots & \ddots & \vdots \\
\mX_{N_t}^{\top}\mX_{1} & \ldots & \mX_{N_t}^{\top} \mX_{N_t}
\end{bmatrix},
\quad
\mS = \begin{bmatrix}
\mX_{1}^{\top}\mX_{1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \mX_{N_t}^{\top} \mX_{N_t}
\end{bmatrix}
\quad
\textrm{and}
\quad
\vw = \begin{bmatrix}
\vw_1 \\ \vdots \\ \vw_{N_t}
\end{bmatrix}
\end{align*}
Thus, $\mR$ is the inter-trial block covariance matrix which captures sub-covariance matrices between all pairs of trials $n\in \{1, \dots, N_t \}$. $\mS$ is a block diagonal matrix which captures within-set covariance matrices for each trial. $\vw$ is a matrix of optimal spatial filters (vectors) $\vw_1, \dots, \vw_{N_t}$ resulting from the largest combined canonical correlation between all canonical variables $\vz_i = \mX_i\vw_i \in \R^{N_s}, \; \forall \; i\in[1, \; N_t]$. As with standard CCA, the largest canonical correlation $\rho^*$ corresponds to the largest generalised eigenvalue in (\ref{eq:mset-cca-gen-eig}). The canonical variables $\vz_i$ corresponding to $\rho^*$ are indeed the eigenvectors corresponding to the largest generalised eigenvalue \cite{zhang-mset-cca}. The optimal reference $\mY_k \in \R^{N_s \times N_t}$ for given frequency $f_k$ can be computed using the spatial filters from $\vw$ as:
\begin{align}
    \mY_k =
    \begin{bmatrix}
    \vz_{1, \, k} & \dots & \vz_{N_t, \,k}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \mX_{1, \, k}\vw_{1, \, k}  & \dots & \mX_{N_t, \,k}\vw_{N_t,\, k} 
    \end{bmatrix}
\end{align}

After this training or calibration process is complete and $\mY_k$ is computed for all candidate frequencies $f_k$, these optimised reference sets can be used for inference. Given a new set of test signals $\hat{\mX} \in \R^{N_s \times N_c}$, ordinary CCA as in Section \ref{section:cca-c3} can be used to discern the frequency $f^*$ corresponding to the highest canonical correlation between $\hat{\mX}$ and the associated referenced set $\mY^*$. The process for finding $f^*$ is identical to that in (\ref{eq:cca-freq-discrimination}). The important difference here is how the pre-computed reference sets $\mY_k$ used in the CCA algorithm are calculated.

\subsection{Generalised CCA (GCCA)}
Consider a single candidate frequency $f_k$: for the rest of this subsection, the $k$ index is ommitted for brevity but all symbols refer to those specific to the single set of trials for $f_k$. As with MsetCCA, GCCA requires a signal tensor $\tX \in \R^{N_s \times N_c \times N_t}$ that incorporates several trials. The \textit{template matrix} $\overline{\mX}$ is obtained by computing the arithmetic mean of all trials in the set: $\overline{\mX}= \frac{1}{N_t}\sum_{i=1}^{N_t}\tX_{i}$. The concatenated signal matrix $\mX^{c}$ is formed by unrolling $\tX$ along the third (trial) axis:
\begin{equation}
    \mX^{c} = \begin{bmatrix}
    \mX_1^\top & \mX_2^\top & \dots & \mX_{N_t}^\top
    \end{bmatrix} \in \R^{N_c \times (N_s N_t)} 
\end{equation}
Then, the template and sinusoidal reference matrices are concatenated similarly to match the dimensions of  
$\mX^{c}$:
\begin{equation}
    \overline{\mX}^{c} = \begin{bmatrix}
    \overline{\mX}^\top & \overline{\mX}^\top & \dots & \overline{\mX}^\top
    \end{bmatrix} \in \R^{N_c \times (N_s N_t)} \quad \textrm{and} \quad  \mY^{c} = \begin{bmatrix}
    \mY^\top & \mY^\top & \dots & \mY^\top
    \end{bmatrix} \in \R^{2N_h \times (N_s N_t)}
\end{equation}
where $\mY$ follows the definition of the sinusoidal reference in (\ref{eq:sinusoidal-ref}). Consider the augmented spatial filter vector $\widetilde{\vw}$: 
\begin{equation}
\widetilde{\vw} = \begin{bmatrix}\vw_{\mX^{c}} & \vw_{\overline{\mX}^{c}} & \vw_{\mY^{c}} \end{bmatrix}^\top 
\end{equation}
Similarly, the augmented signal matrix $\widetilde{\mX}$ is defined as: 
\begin{equation}
\widetilde{\mX} = \begin{bmatrix}(\mX^{c})^\top & (\overline{\mX}^{c})^\top & (\mY^{c})^\top \end{bmatrix}^\top
\end{equation}
The objective of GCCA can then be expressed as
\begin{align}
\begin{split}
    \textrm{maximise} & \quad \textrm{tr}(\widetilde{\vw}^\top\widetilde{\mX}\widetilde{\mX}^\top\widetilde{\vw}) \\
    \textrm{s.t.} & \quad \widetilde{\vw}^\top\mD\widetilde{\vw} = \mI
\end{split}
\label{eq:gcca-objective}
\end{align}
where $\mD$, the within-set block covariance matrix, is defined as 
\begin{equation}
    \mD = \begin{bmatrix}
    \mX^{c}(\mX^{c}})^\top & 0 & 0 \\
    0 & \overline{\mX}^{c}(\overline{\mX}^{c})^\top & 0 \\
    0 & 0 & \mY^{c}(\mY^{c}})^\top
    \end{bmatrix}
\end{equation}
Again, using the method of Lagrange multipliers, (\ref{eq:gcca-objective}) can be reformulated as a generalised eigenvalue problem of the form
\begin{equation}
    \widetilde{\mX}\widetilde{\mX}^\top\widetilde{\vw} = \lambda\mD\widetilde{\vw} 
    \label{eq:gcca-D}
\end{equation}
If $\mD$ is non-singular, (\ref{eq:gcca-D}) resolves to an ordinary eigenvalue problem:
\begin{equation}
    \mD^{-1}\widetilde{\mX}\widetilde{\mX}^\top\widetilde{\vw} = \lambda\widetilde{\vw} 
    \label{eq:gcca-D-standard}
\end{equation}
Then, the eigenvector of $\mD^{-1}\widetilde{\mX}\widetilde{\mX}^\top$ corresponding to its largest eigenvalue is the optimal spatial filter $\vw^* \in \R^{2(N_c+N_h)}$. 

Given a new test sample set $\hat{\mX} \in \R^{N_s\times N_c}$, two correlations are computed: first between the test data and the historical template, and then between the test data and sinusoidal reference. This can be expressed as 
\begin{align}
    \rho_1 &= \textrm{corr}(\hat{\mX}\vw_{\mX^{c}}, \; \overline{\mX}\vw_{\overline{\mX}^{c}}) \\
    \rho_2 &= \textrm{corr}(\hat{\mX}\vw_{\mX^{c}}, \; \mY\vw_{\mY^{c}}) 
\end{align}
where Pearson's correlation is denoted by \textit{corr}. Finally, the output correlation for frequency $f_k$ is computed as a combination of $\rho_1$ and $\rho_2$:
\begin{equation}
    \rho = \textrm{sign}(\rho_1)\rho_1^2 + \textrm{sign}(\rho_2)\rho_2^2
\end{equation}


\subsection{Related theory}
\subsubsection{Significance testing}
\subsubsection{Quantifying signal quality}
\subsubsection{Mechanics of sampling}