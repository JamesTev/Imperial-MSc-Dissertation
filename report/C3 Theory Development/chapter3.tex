% Chapter 3: Theory Development
\chapter{Theory Development}
\label{chapter:theory-development}

\graphicspath{ {report/C3 Theory Development/assets/} } 

\section{Eigenvalue Optimisation}
Many, if not all, of the statistical decoding algorithms mentioned so far require a form of eigenvalue optimisation to find a solution. Worded differently, these are optimisation problems that can be conveniently reformulated as eigenvalue problems. 

\subsection{Optimisation form I}
\label{subsection:eig-opt-form-1}
This form addresses constrained optimisation problems of the form

\begin{equation}
\begin{array}{ll}
\underset{\vw}{\operatorname{maximize}} & \vw^{\top} \mA \vw \\
\text { subject to } & \vw^{\top} \vw=1,
\end{array}
\label{eq:eig-opt-1}
\end{equation}
with variable $\vw \in \R^{d}$ and $\mA \in \R^{d\times d}$. Employing Lagrange dual theory, the Lagrangian for (\ref{eq:eig-opt-1}) is 
\begin{equation}
\mathcal{L}=\vw^{\top} \boldsymbol{A} \vw-\lambda\left(\vw^{\top} \vw-1\right),
\end{equation}
with Lagrange multiplier $\lambda \in \R$ \cite{eig-tutorial}. Referencing the KKT conditions, \textit{stationary} requires $\frac{\partial \mathcal{L}}{\partial \vw} = 0$:
\begin{equation}
\mathbb{R}^{d} \ni \frac{\partial \mathcal{L}}{\partial \vw}=2 A \vw-2 \lambda \vw \stackrel{\text { set }}{=} 0 \Longrightarrow A \vw=\lambda \vw,
\end{equation}
yielding the standard eigenvalue problem in $\mA$. Since (\ref{eq:eig-opt-1}) was posed as a maximisation problem, the optimal $\vw$ corresponds to the largest eigenvalue in $\mA$. For a minimisation problem, it would correspond to the smallest eigenvalue.

\subsection{Optimisation form II}
\label{subsection:eig-opt-form-2}
The second form is an extension of the prior univariate case to a multivariable system $\mW \in \R^{d\times d}$. Consider the following constrained optimisation problem:

\begin{equation}
\begin{array}{ll}
\underset{\boldsymbol{\mW}}{\operatorname{maximize}} & \operatorname{tr}\left(\mW^{\top} \mA \boldsymbol{\mW}\right) \\
\text { subject to } & \boldsymbol{\mW}^{\top} \boldsymbol{\mW}=\boldsymbol{\mI}\, ,
\end{array}
\label{eq:eig-opt-2}
\end{equation}

where tr(.) represents the matrix trace and $\mA \in \R^{d\times d}$. The Lagrangian for this problem, taken from
\cite{eig-tutorial}, is the following:

\begin{equation}
    \mathcal{L} = \text{tr}(\mW^\top \mA \mW) - \text{tr}(\Lambda^\top(\mW^\top\mW - \mI)),
\end{equation}
where $\Lambda \in \R^{d\times d}$ is a diagonal matrix whose diagonal entries are the Lagrangian multipliers. Once again, applying the \textit{stationarity} condition, (\ref{eq:eig-opt-2}) can be reformulated to the following eigenvalue problem \cite{eig-tutorial}:
\begin{align}
\mathbb{R}^{d \times d} \ni \frac{\partial \mathcal{L}}{\partial \mW}&=2 A \mW -2 \mW \Lambda \stackrel{\text { set }}{=} 0 \\
\Longrightarrow A \mW &=\mW \Lambda
\end{align}
The diagonal elements of $\Lambda$ are the eigenvalues of $\mA$ and are arranged in descending order for the maximisation problem and ascending order for the minimisation problem. The columns of $\mW$ are the corresponding eigenvectors.

\subsection{Power iteration}
\label{subsection:power-iteration-c3}
The power iteration algorithm is a simple iterative algorithm used to find the eigenvector $\vq_1 \in \R^d$ corresponding to the largest eigenvalue of a matrix $A\in \R^{d\times d}$ \cite{panju-iterative-eig}. It relies on the property that if $\vq$ is an eigenvector of matrix $\mA$ with eigenvalue $\lambda$, then $\mA^k \vq = \lambda^k\vq, \quad k\in \mathbb{N}$. Let unit vector $\vv^{(0)} \in \R^d$ be the initial estimate of an eigenvector of $\mA$. Assuming that $\mA$ is full rank with real eigenvalues, its eigenvectors $\vq_i$ are orthogonal and form a basis for $\R^d$. Therefore, $\vv^{(0)}$ can be expressed as a linear combination of $\vq_i$:
\begin{equation}
    \vv^{(0)} = \sum_{i=1}^{d} c_i\vq_i \quad \text{for} \quad c_i \in \R, \, \forall i
\label{eq:power-iter-lin-sum}
\end{equation}
Assuming that $c_1 \neq 0$:
\begin{equation}
\begin{aligned}
\mA \vv^{(0)} &= \sum_{i=1}^{d} c_i\lambda_i\vq_i \implies
\mA^{k} \vv^{(0)} = \sum_{i=1}^{d} c_i\lambda_i^k\vq_i
\end{aligned}
\end{equation}
Then, taking $\lambda_1^k$ as a common factor,
\begin{equation}
   \mA^{k} \vv^{(0)} = \lambda_{1}^{k}\left(c_{1} \vq_{1}+c_{2}\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{k} \vq_{2}+\ldots+c_{n}\left(\frac{\lambda_{n}}{\lambda_{1}}\right)^{k} \vq_{n}\right)
\end{equation}
Assuming that the eigenvalues $\lambda_i$ are assumed to be real, distinct and descending in magnitude, it is clear to see that for $i = 2, \dots, d$,
\begin{equation}
    \lim _{k \rightarrow \infty}\left(\frac{\lambda_{i}}{\lambda_{1}}\right)^{k}=0
\end{equation}
Thus, over multiple iterations (increasing $k$), $\mA \vv^{(0)} \rightarrow c_1\lambda_1^k \vq_1$ and $\vv^{(k)}$ approaches the eigenvector $\vq_1$:
\begin{equation}
    \vq_1 \approx \frac{\mA \vv^{(0)} }{\|\mA \vv^{(0)} \|}
\end{equation}
An approximation of the eigenvalue corresponding to $\vq_1$ can be found using the Rayleigh quotient for $\vv^{(k)}$ defined as 
\begin{equation}
    r(\vv) = \frac{\vv^\top \mA \vv}{\vv^\top \vv} \approx \frac{\lambda \vv^\top \vv}{\vv^\top \vv} = \lambda,
\label{eq:rayleigh-quotient}
\end{equation}
where the iteration index $k$ has been omitted for readability. Although elegant, this algorithm is limited in that it only returns a single eigenvector and only converges if eigenvalues $\lambda_1, \lambda_2, \dots$ are \textit{distinct}. However, for problems that only require solving of the largest eigenvalue/eigenvector, this is a useful algorithm that is easy to implement.

\subsection{Simultaneous iteration}
The power iteration algorithm, along with its extensions such as the inverse power iteration and Rayleigh Quotient iteration algorithms, only compute a single eigenvector/eigenvalue at a time. In order to compute several eigenvalues, these methods must be reapplied several times \cite{panju-iterative-eig}. Simultaneous iteration offers a method of solving all eigenvalues at once. 

As in (\ref{eq:power-iter-lin-sum}) in the power iteration algorithm, only non-trivial eigenvectors in the linear sum with $c_i \neq 0$ can be found by the algorithm. Equivalently, only eigenvectors \textit{not orthogonal} to $\vv^{(0)}$ can be selected by the algorithm. This suggests that if different, mutually-orthogonal initial vectors $\vv^{(0)}$ are selected across runs, there is a chance of finding different eigenvalues \cite{panju-iterative-eig}. Using this idea, instead of beginning with $\vv^{(0)}$, consider an initial basis of $d$ linearly independent vectors arranged as the columns of a matrix $\mV^{(0)}$:
\begin{equation}
    \mV = \left[\begin{array}{l|l|l}\vv_1^{(0)} & \dots & \vv_d^{(0)}\end{array}\right]
\end{equation}
If we let
\begin{equation}
       \mV^{(k)} = \mA^{(k)}\mV^{(0)} = \left[\begin{array}{l|l|l}\vv_1^{(k)} & \dots & \vv_d^{(k)}\end{array}\right],
\end{equation}
the standard power iteration is effectively applied to all the vectors $\vv_1^{(0)}, \dots, \vv_d^{(0)}$ at once \cite{panju-iterative-eig}. However, in order to actually extract different eigenvectors from the set of distinct initial vectors (columns of $\mV$), we must orthonormalise the columns of $\mV^{(k)}$ at each iteration. This was done in the univariate version of the power iteration algorithm; the multivariate analogue is to ensure the set of eigenvector estimates (columns of $\mV^{(k)}$ are orthonormal in each iteration \cite{panju-iterative-eig}. This is achieved by using the QR decomposition of $\mV^{(k)}$ which decomposes a matrix into the product of an orthonormal matrix $\mQ$ and an upper triangular matrix $\mR$. In each iteration, a new matrix $\mW$ is obtained by multiplying $\mA$ by the latest eigenvector approximation matrix $\mV^{(k-1)}$. $\mW$ is updated to the orthonormal columns of $\mQ^{(k)}$ from the QR decomposition of $\mW^{(k-1)}$.

\subsection{QR iteration algorithm}
While the simultaneous iteration is effective under some conditions, it is not commonly used in practice \cite{panju-iterative-eig}. The QR algorithm is a subtle yet elegant adjustment: in each iteration, $\mA$ is modified directly. Specifically, factor matrices $\mQ$ and $\mR$ from the QR decomposition of $\mA^{(k-1)}$ are simply multiplied in reverse order to obtain $\mA^{(k)} = \mR^{(k)}\mQ^{(k)}$. The motivation behind this is not immediately obvious or intuitive but can be explained through a restructuring of the simultaneous iteration algorithm as detailed at length in \cite{panju-iterative-eig}. 

It should be noted that both the simultaneous iteration and QR iteration algorithms may fail if the eigenvalues of $\mA$ are not real or if the eigenvectors thereof do not form a basis for $\R^d$. 

\section{SSVEP Decoding Algorithms}
As mentioned in Section \ref{subsection:psda-vs-cca}, statistical algorithms selected or designed for SSVEP decoding have proven to be significantly more successful than PSDA approaches. Therefore, only the former will be explored in this section.

\subsection{Canonical correlation analysis (CCA)}
\label{section:cca-c3}
Given variable partitions $\mX \in \R^{m\times p}$, $\mY \in \R^{m\times q}$ each with $m$ observations, CCA seeks to find a linear combination of $\mX$, $\mY$ that maximises the correlation between their images. More formally, assuming that the variables (columns) of the two partitions $\vx_1, \dots, \vx_p \in \R^{m}$ and $\vy_1, \dots, \vy_q \in \R^{m}$ are standardised to have zero mean and unit variance, find
\begin{align}
    \rho &= \max_{\vw_X, \vw_Y} \textrm{corr}(\mX \vw_X, \mY \vw_Y) \\
    &= \max_{\vw_X, \vw_Y} \frac{\E[\mX \vw_X\vw_Y^\top\mY^\top ]}{\sqrt{\E[\mX \vw_X\vw_X^\top\mX^\top]\E[\mY \vw_Y\vw_Y^\top\mY^\top]}}
    \label{eq:cca-corr-objective}
\end{align}
where $\rho$ is the \textit{canonical correlation}, $\vw_X$ and $\vw_Y$ are the canonical weight vectors and the images $\vz_X = \mX\vw_X$ and $\vz_Y = \mY\vw_Y$ are the \textit{canonical variables} for $\mX$ and $\mY$ respectively.

\subsubsection{Geometric interpretation}
The authors in \cite{cca-tutorial} present an intuitive geometric analogy. Consider $\mX$ and $\mY$ as linear transforms of position vectors $\vw_X$ and $\vw_Y$ onto their images $\vz_X$ and $\vz_Y$ in $\R^m$. CCA constrains $\vz_X$ and $\vz_Y$ to have unit norm and that the angle $\theta \in [0, \frac{\pi}{2}]$ between them be minimised so as to maximise their correlation. The canonical correlation $\rho$ is then the inner product of $\vz_X$ and $\vz_Y$:
\begin{equation}
    \rho = \vz_X^\top\vz_Y = \|\vz_X\| \|\vz_Y\|\cos(\theta) = \cos(\theta)
\end{equation}
Therefore, CCA aims to find position vectors $\vw_X$, $\vw_Y$ that, after undergoing linear transforms $\mX$ and $\mY$, are mapped to a unit sphere in $\R^m$ where $\theta$ is minimised \cite{cca-tutorial}. Note that there will be $n=\textrm{min}(p, q)$ canonical correlations $\rho_1, \dots, \rho_n$ with $|\rho_i| \geq |\rho_j|, \; \forall \; i < j$. Typically, only the first and largest canonical correlation $\rho_1$ is considered. 

\subsubsection{Solving for canonical weight vectors through the eigenvalue problem}
Consider the case of solving for the first and largest canonical correlation $p_1$. Following the geometric interpretation, the objective is to find
\begin{equation}
    \cos(\theta_1) = \max_{\vw_X, \vw_Y}\langle\,\vz_X,\vz_Y\rangle \quad \textrm{with} \quad \|\vz_X\| = \|\vz_Y\| = 1
\label{eq:cca-objective-1}
\end{equation}
where $\theta_1$ is the smallest angle between image vectors. Defining the within-set covariance matrices as 
\begin{align}
    \mC_{XX} = \mX^\top\mX \quad \textrm{and} \quad
    \mC_{YY} = \mY^\top \mY 
\end{align}
and the inter-set cross covariance matrix as 
\begin{align}
    \mC_{XY} &= \mX^\top\mY,
\end{align}
the unit variance constraint on the image vectors can be rewritten as  \begin{align}
\begin{split}
    \|\vz_X\| &= \vz_X^\top\vz_X = \vw_X^\top \mX^\top \mX \vw_X =  \vw_X^\top \mC_{XX} \vw_X = 1 \\
    \|\vz_Y\| &= \vz_Y^\top\vz_Y = \vw_Y^\top \mY^\top \mY \vw_Y =  \vw_Y^\top \mC_{YY} \vw_Y = 1 
    \end{split}
\label{eq:unit-variance-reformulated}
\end{align}
Substituting (\ref{eq:unit-variance-reformulated}) into (\ref{eq:cca-corr-objective}) yields 

\begin{equation}
\cos(\theta) = \max_{\vw_X, \vw_Y} \vw_X^\top \mC_{XY} \vw_Y 
\end{equation}
Lagrangian dual theory provides a straight forward way to solve the constrained optimisation problem above with unit variance constraints as in (\ref{eq:unit-variance-reformulated}). Consider the Lagrangian for this problem below with Lagrange multipliers $\lambda_1$ and $\lambda_2$:
\begin{equation}
    \mathcal{L}(\vw_X, \vw_Y, \lambda_1, \lambda_2) = \vw_X^\top \mC_{XY} \vw_Y - \frac{\lambda_1}{2}(\vw_X^\top \mC_{XX} \vw_X-1) - \frac{\lambda_2}{2}(\vw_Y^\top \mC_{YY} \vw_Y-1)
\end{equation}
With reference to the KKT conditions, stationary requires
\begin{align}
    \label{eq:cca-lagrange-x}
    \frac{\partial \mathcal{L}}{\partial \vw_X} &= \mC_{XY} \vw_Y - \lambda_1\mC_{XX} \vw_X = 0 \\
    \frac{\partial \mathcal{L}}{\partial \vw_Y} &= \mC_{YX} \vw_X - \lambda_2\mC_{YY} \vw_Y = 0
        \label{eq:cca-lagrange-y}
\end{align}
Since $\mC_{YX} = \mC_{XY}^\top$. Pre-multiplying (\ref{eq:cca-lagrange-x}) by $\vw_X^\top$ and (\ref{eq:cca-lagrange-y}) by $\vw_Y^\top$ yields:
\begin{align}
\begin{split}
    \vw_X^\top\mC_{XY} \vw_Y - \lambda_1\vw_X^\top\mC_{XX} \vw_X &= 0 \\
    \vw_Y^\top\mC_{YX} \vw_X - \lambda_2\vw_Y^\top\mC_{YY} \vw_Y &= 0
\end{split}
\label{eq:lagrange-multi}
\end{align}
Since $\vw_X^\top \mC_{XX} \vw_X = \vw_Y^\top \mC_{YY} \vw_Y =1$, it is clear from (\ref{eq:lagrange-multi}) that $\lambda_1=\lambda_2 = \lambda$. Substituting this result into (\ref{eq:cca-lagrange-x}) yields
\begin{align}
    \vw_X = \frac{\mC_{XX}^{-1}C_{XY}\vw_Y}{\lambda}
\end{align}
Then, substituting into (\ref{eq:cca-lagrange-y}) produces the following generalised eigenvalue problem:
\begin{align}
    \mC_{YX}\mC^{-1}_{XX}\mC_{XY}\vw_{Y} = \lambda^2\mC_{YY}\vw_Y
\end{align}
If $\mC_{YY}$ is non-singular, this reduces to a standard eigenvalue problem of the form
\begin{align}
    \mC_{YY}^{-1}\mC_{YX}\mC^{-1}_{XX}\mC_{XY}\vw_{Y} = \lambda^2\vw_Y
\end{align}
The eigenvalues of the matrix $\mC_{YY}^{-1}\mC_{YX}\mC^{-1}_{XX}\mC_{XY}$ correspond to the squares of the canonical correlations $\rho_1, \dots, \rho_n$.

\subsection{Task-related component analysis (TRCA)}
\label{sec:trca-c3}
Expanding from the toy example in Section \ref{section:trca-c2}, the TRCA algorithm implicitly assumes that observed signals are generated as a linear combination of task-related and task-unrelated components which implies that task-related components can be recovered by applying an appropriate weighting of observed signals across trials \cite{tanaka-trca}. Inter-trial covariance maximisation is the core objective that aims to extract task-related components with maximal temporal similarity across trials. 

Now that multiple trials are to be considered, the signal matrix $\mX\in \R^{N_s \times N_c}$ introduced in Section \ref{section:cca-c3} must be extended to a third order signal tensor $\tX \in \R^{N_s \times N_c \times N_t}$ with $N_t$ trials, each with $N_c$ channels\footnote{the constraint that the number of variables (width of $\mX_i$) for each trial be constant is not strictly necessary. For this application, however, it is assumed that the number of channels will not change between trials of a given experiment.} and $N_s$ samples. Let $\vy^{(k)}$ be the $k$-th trial (block) with $k\in\{1, 2, \dots, N_t\}$ of the signal $y(t)$ and $\vx_i^{(k)}$ be the $k$-th trial of the input signal from channel $i, \, i\in\{1, \dots, N_c\}$. Consider the covariance between all $i$ channels for two trials $k$ and $l$:

\begin{equation}
    \widehat{\mC}_{k l}=\operatorname{Cov}\left(\vy^{(k)}, \vy^{(l)}\right)=\sum_{i, \, j=1}^{N_c} w_{i} w_{j} \operatorname{Cov}\left(\vx_{i}^{(k)}, \vx_{k}^{(l)}\right)
\end{equation}
Furthermore, as with CCA, we must impose the constraint on the designed weights such that the variance of $y(t)$ is bounded:
\begin{equation}
    \operatorname{Var}(\vy)=\sum_{i, \, j=1}^{N_c} w_{i} w_{j} \operatorname{Cov}\left(\vx_{i}, \vx_{j}\right)=\mathbf{w}^{\mathrm{T}} \mQ \mathbf{w}=1
\end{equation}
Then, all possible combinations of trials $1, \dots, N_t$ are summed as
\begin{align}
\begin{split}
\sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \widehat{\mC}_{k l} &=\sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \operatorname{Cov}\left(\vy^{(k)}, \vy^{(l)}\right) \\
& =\sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \sum_{i, j=1}^{N_c} w_{i} w_{j} \operatorname{Cov}\left(\vx_{i}^{(k)}, \vx_{j}^{(l)}\right)= \vw^\top \mS \vw
\end{split}
\label{eq:trca-cov-sum}
\end{align}
where $\mS$ is the following symmetric matrix
\begin{equation}
    \mS_{i, j} = \sum_{\substack{k, \, l=1, \\ k\neq l}}^{N_t} \operatorname{Cov}\left(\vx_{i}^{(k)}, \vx_{j}^{(l)}\right)
\end{equation}
As alluded to before, (\ref{eq:trca-cov-sum}) yields the sum of covariances of all combinations of trials: a measure of task consistency. This is precisely what we want to maximise. This constrained optimisation problem (recalling the variance constraint on $y(t)$) can be reformulated as a Rayleigh-Ritz eigenvalue problem of the form 

\begin{equation}
    \vw^*=\underset{\vw}{\arg \max } \frac{\vw^\top \mS \vw}{\vw^\top \mQ \vw}
\label{eq:trca-rayleigh-ritz}
\end{equation}
The inter-trial weight vectors $\vw_k, \, k \in \{1, \dots, N_t\}$ can be found by computing the eigenvectors of $\mQ^{-1}\mS$. Specifically, the degree of task-relatedness corresponds to the magnitude of the eigenvalues of $\mQ^{-1}\mS$ and so the optimal weight $\vw^*$ corresponds to the largest eigenvalue.

\subsection{Multiset CCA (MsetCCA)}
MsetCCA is an extension of standard CCA to multiple data sets or partitions. Accordingly, the objective is now to maximise the correlation between canonical variables from \textit{many sets} of observations at a given stimulus frequency $f_k$. Although several objective functions for MsetCCA exist, the MAXVAR objective is explored in \cite{zhang-mset-cca} for its intuitive extension to ordinary CCA with multiple variable sets. Assuming all sets of variables in $\tX$ are normalised to have zero mean and unit variance, the MAXVAR objective for maximising correlation over canonical variables from multiple sets at a given candidate frequency $f_k$ is defined as follows:
\begin{align}
\begin{split}
\max_{\mathbf{w}_{1}, \ldots, \mathbf{w}_{N_t}} & \quad \rho=\sum_{i \neq j}^{N_t} \mathbf{w}_{i}^\top\mX_{i}^\top \mX_{j}\mathbf{w}_{j} \\
\text { s.t. } & \quad \frac{1}{N_t} \sum_{i=1}^{N_t} \mathbf{w}_{i}^\top\mX_{i}^\top \mX_{i}\mathbf{w}_{i}=1,
\end{split}
\label{eq:mset-cca-objective}
\end{align}
where the same conventions for the signal tensor $\tX$ as in Section \ref{sec:trca-c3} above apply. Following a similar approach as in the derivation for CCA above, the method of Lagrange multipliers can be used to transform the constrained optimisation problem in (\ref{eq:mset-cca-objective}) to a generalised eigenvalue problem of the form:
\begin{equation}
        (\mR -\mS)\vw = \rho\mS \vw
\label{eq:mset-cca-gen-eig}
\end{equation}
where
\begin{align*}
\mR = \begin{bmatrix}
\mX_{1}^{\top}\mX_{1} & \ldots & \mX_{1}^{\top}\mX_{N_t} \\
\vdots & \ddots & \vdots \\
\mX_{N_t}^{\top}\mX_{1} & \ldots & \mX_{N_t}^{\top} \mX_{N_t}
\end{bmatrix},
\quad
\mS = \begin{bmatrix}
\mX_{1}^{\top}\mX_{1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \mX_{N_t}^{\top} \mX_{N_t}
\end{bmatrix}
\quad
\textrm{and}
\quad
\vw = \begin{bmatrix}
\vw_1 \\ \vdots \\ \vw_{N_t}
\end{bmatrix}
\end{align*}
Thus, $\mR$ is the inter-trial block covariance matrix which captures sub-covariance matrices between all pairs of trials $n\in \{1, \dots, N_t \}$. $\mS$ is a block diagonal matrix which captures within-set covariance matrices for each trial. $\vw$ is a matrix of optimal spatial filters (vectors) $\vw_1, \dots, \vw_{N_t}$ resulting from the largest combined canonical correlation between all canonical variables $\vz_i = \mX_i\vw_i \in \R^{N_s}, \; \forall \; i\in[1, \; N_t]$. As with standard CCA, the largest canonical correlation $\rho^*$ corresponds to the largest generalised eigenvalue in (\ref{eq:mset-cca-gen-eig}). The canonical variables $\vz_i$ corresponding to $\rho^*$ are indeed the eigenvectors corresponding to the largest generalised eigenvalue \cite{zhang-mset-cca}. The optimal reference $\mY_k \in \R^{N_s \times N_t}$ for given frequency $f_k$ can be computed using the spatial filters from $\vw$ as:
\begin{align}
    \mY_k =
    \begin{bmatrix}
    \vz_{1, \, k} & \dots & \vz_{N_t, \,k}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \mX_{1, \, k}\vw_{1, \, k}  & \dots & \mX_{N_t, \,k}\vw_{N_t,\, k} 
    \end{bmatrix}
\end{align}

After this training or calibration process is complete and $\mY_k$ is computed for all candidate frequencies $f_k$, these optimised reference sets can be used for inference. Given a new set of test signals $\hat{\mX} \in \R^{N_s \times N_c}$, ordinary CCA as in Section \ref{section:cca-c3} can be used to discern the frequency $f^*$ corresponding to the highest canonical correlation between $\hat{\mX}$ and the associated referenced set $\mY^*$. The process for finding $f^*$ is identical to that in (\ref{eq:cca-freq-discrimination}). The important difference here is how the pre-computed reference sets $\mY_k$ used in the CCA algorithm are calculated.

\subsection{Generalised CCA (GCCA)}
Consider a single candidate frequency $f_k$: for the rest of this subsection, the $k$ index is ommitted for brevity but all symbols refer to those specific to the single set of trials for $f_k$. As with MsetCCA, GCCA requires a signal tensor $\tX \in \R^{N_s \times N_c \times N_t}$ that incorporates several trials. The \textit{template matrix} $\overline{\mX}$ is obtained by computing the arithmetic mean of all trials in the set: $\overline{\mX}= \frac{1}{N_t}\sum_{i=1}^{N_t}\tX_{i}$. The concatenated signal matrix $\mX^{c}$ is formed by unrolling $\tX$ along the third (trial) axis:
\begin{equation}
    \mX^{c} = \begin{bmatrix}
    \mX_1^\top & \mX_2^\top & \dots & \mX_{N_t}^\top
    \end{bmatrix} \in \R^{N_c \times (N_s N_t)} 
\end{equation}
Then, the template and sinusoidal reference matrices are concatenated similarly to match the dimensions of  
$\mX^{c}$:
\begin{equation}
    \overline{\mX}^{c} = \begin{bmatrix}
    \overline{\mX}^\top & \overline{\mX}^\top & \dots & \overline{\mX}^\top
    \end{bmatrix} \in \R^{N_c \times (N_s N_t)} \quad \textrm{and} \quad  \mY^{c} = \begin{bmatrix}
    \mY^\top & \mY^\top & \dots & \mY^\top
    \end{bmatrix} \in \R^{2N_h \times (N_s N_t)}
\end{equation}
where $\mY$ follows the definition of the sinusoidal reference in (\ref{eq:sinusoidal-ref}). Consider the augmented spatial filter vector $\widetilde{\vw}$: 
\begin{equation}
\widetilde{\vw} = \begin{bmatrix}\vw_{\mX^{c}} & \vw_{\overline{\mX}^{c}} & \vw_{\mY^{c}} \end{bmatrix}^\top 
\end{equation}
Similarly, the augmented signal matrix $\widetilde{\mX}$ is defined as: 
\begin{equation}
\widetilde{\mX} = \begin{bmatrix}(\mX^{c})^\top & (\overline{\mX}^{c})^\top & (\mY^{c})^\top \end{bmatrix}^\top
\end{equation}
The objective of GCCA can then be expressed as
\begin{align}
\begin{split}
    \textrm{maximise} & \quad \textrm{tr}(\widetilde{\vw}^\top\widetilde{\mX}\widetilde{\mX}^\top\widetilde{\vw}) \\
    \textrm{s.t.} & \quad \widetilde{\vw}^\top\mD\widetilde{\vw} = \mI
\end{split}
\label{eq:gcca-objective}
\end{align}
where $\mD$, the within-set block covariance matrix, is defined as 
\begin{equation}
    \mD = \begin{bmatrix}
    \mX^{c}(\mX^{c}})^\top & 0 & 0 \\
    0 & \overline{\mX}^{c}(\overline{\mX}^{c})^\top & 0 \\
    0 & 0 & \mY^{c}(\mY^{c})^\top
    \end{bmatrix}
\end{equation}
Again, using the method of Lagrange multipliers, (\ref{eq:gcca-objective}) can be reformulated as a generalised eigenvalue problem of the form
\begin{equation}
    \widetilde{\mX}\widetilde{\mX}^\top\widetilde{\vw} = \lambda\mD\widetilde{\vw} 
    \label{eq:gcca-D}
\end{equation}
If $\mD$ is non-singular, (\ref{eq:gcca-D}) resolves to an ordinary eigenvalue problem:
\begin{equation}
    \mD^{-1}\widetilde{\mX}\widetilde{\mX}^\top\widetilde{\vw} = \lambda\widetilde{\vw} 
    \label{eq:gcca-D-standard}
\end{equation}
Then, the eigenvector of $\mD^{-1}\widetilde{\mX}\widetilde{\mX}^\top$ corresponding to its largest eigenvalue is the optimal spatial filter $\vw^* \in \R^{2(N_c+N_h)}$. 

Given a new test sample set $\hat{\mX} \in \R^{N_s\times N_c}$, two correlations are computed: first between the test data and the historical template, and then between the test data and sinusoidal reference. This can be expressed as 
\begin{align}
    \rho_1 &= \textrm{corr}(\hat{\mX}\vw_{\mX^{c}}, \; \overline{\mX}\vw_{\overline{\mX}^{c}}) \\
    \rho_2 &= \textrm{corr}(\hat{\mX}\vw_{\mX^{c}}, \; \mY\vw_{\mY^{c}}) 
\end{align}
where Pearson's correlation is denoted by \textit{corr}. Finally, the output correlation for frequency $f_k$ is computed as a combination of $\rho_1$ and $\rho_2$:
\begin{equation}
    \rho = \textrm{sign}(\rho_1)\rho_1^2 + \textrm{sign}(\rho_2)\rho_2^2
\end{equation}


% \subsection{Related theory}
% \subsubsection{Significance testing}
% \subsubsection{Quantifying signal quality}
% \subsubsection{Mechanics of sampling}