%% Chapter 7: Discussion and Conclusion
\chapter{Discussion and Conclusion}
\label{chapter:discussion-conclusion}

\graphicspath{ {report/Chapter7/assets/} } 

\section{Discussion}
% Lessons learnt

\subsection{Digital signal processing system}
The role of the DSP system in this project was paramount. Without appropriately processed signals, even the most sophisticated decoding algorithms would have been rendered useless. 

The time domain plot in Figure \ref{fig:sq-wave-time-c6} demonstrates effective sampling; the input square wave is captured with consistent frequency and amplitude. As is more visible in Figure \ref{fig:sq-wave-spectra-c6}, the estimated spectrum of the low-pass-filtered signal is almost exactly as desired; pass-band distortion is negligible and extremely steep roll-off is achieved in the transition band. Furthermore, stop-band attenuation is more than sufficient and, encouragingly, is very close to the 80dB design specification. Finally, after establishing effective sampling and filtering, the PSD estimate of the downsampled signal in Figure \ref{fig:sq-wave-ds-spectra-c6} verifies that the theoretical downsampling rate of $f_s'=\frac{1}{4}f_s = 64$Hz is able to maintain signal fidelity in the pass-band. The implication of this is that 4x fewer samples can be used to represent the original signal with negligible loss of information. This is invaluable for a system such as the one in this project that is severely constrained in memory and computational power.

In summary, the DSP system proved very effective and was verified to operate in very close agreement with its design parameters. Worth emphasising is the fact that all necessary DSP operations are able to be performed in firmware on the ESP32. Not only is this a prerequisite for the end-to-end on-device processing objective, but also, it makes for a more flexible and future-proof system. New filter weights or other DSP-related parameters can simply be flashed onto the device, or indeed, uploaded `over the air' through a network connection if needed.
\subsection{Decoding}

\subsubsection{Online decoding}
One of the biggest challenges of this project was to develop a system capable of end-to-end, real-time on-device decoding. That is, an embedded system capable of performing all operations needed to acquire, process, decode and communicate signals without outsourcing any computation off the device. 

As documented in the system design in Chapter \ref{chapter:system-design} and demonstrated by the results in Chapter \ref{chapter:results}, one can conclude that this objective has been achieved. This is extremely significant as it means that a fully functional (albeit fairly basic) BCI device can be deployed in a small, self-contained package that is both completely mobile and extremely cost-effective. From a technical perspective, the fact that sophisticated decoding algorithms leveraging eigenvalue solvers and other high level linear algebra can be performed on a microcontroller - and with such ease of implementation - is quite remarkable.  

\subsubsection{Generalisation ability}
One of the most significant limitations of this system is the need for calibration during prior to each new recording session. A session, in this context, is defined as a distinct, continuous period in which the BCI headset was not removed or adjusted. Correspondingly, in order to separate sessions and truly test generalisation ability, it was required that the headset be removed between sessions. The results of the generalisation tests in Section \ref{subsection:generalisation-testing} show that using calibration data from one session was not helpful when used in preparation for inference in a different session. Referring to the convention explained in Section \ref{subsection:generalisation-testing}, Figure \ref{fig:gt-results-gcca} and Figure \ref{fig:gt-results-mcca} show very poor decoding performance for the case of no overlapping trials. This suggests that calibration and test data from two completely distinct sessions is not feasible. Interestingly, with $n>0$ overlapping trials, decoding accuracy was \textit{worse} than in the original decoding tests in Section \ref{subsection:decoding-acc-ns-effect} where calibration and test data sets were drawn from the same session (ultimately leaving less data available for calibration). In the case of overlapping trials, calibration was performed on both the whole first session \textit{and} a subset of the second session on which testing was performed. It is thus surprising that this overlapping case yielded poorer performance as the decoding algorithms had access to a superset of the calibration data available in the initial tests in Section \ref{subsection:decoding-acc-ns-effect}. 

These findings suggest that there is dramatic variance in recorded signals across sessions. This may be due to several factors:
\begin{enumerate}
    \item There was \textbf{no standardised positioning} of the BCI headband on the subject's head across sessions. Each time the headband was put on, an attempt was made to estimate consistent positioning but as the electrodes were positioned at the back of the head, this was difficult to do reliably without assistance (which was seldom available). Consequent deviations in the electrode positions across sessions would naturally produce noticeable variance in measured signals.
    \item Only dry electrodes could be used and \textbf{scalp-contact quality was inconsistent} at best. On some occasions, it was found that electrodes would be lodged in a more favourable orientation with better scalp contact than on others. This was caused by variations in hair partings and angle-of-location relative to the protruding inion at the back of the skull. 
    \item Albeit a differential measurement across two electrodes, only a \textbf{single EEG channel} was available. This poses more of a challenge for decoding algorithms to reject task-unrelated artefacts and random noise owing to having no other independent measurements available.  
\end{enumerate}

\subsection{Networking and communication}
Mention somewhere that the networking aspect with MQTT can stream data up to 5Hz but was only required to do so at around 1Hz (2s sampling window with 50\% overlap)

\subsection{Limitations of the system and experimental procedures}



\subsection{Challenges encountered}
Discuss numerical challenges, precision issues, memory constraints
Discuss some of the auxiliary computational approaches used such as eigenvalue solvers etc


\subsection{Impact of constraints}
Besides the extremely limited budget, the constraint of having only a single channel likely proved to be the most significant in this project. The literature cited in Section \ref{section:existing-bci-tech-c2} suggests that very few, if any, BCI devices rely on a single measurement channel alone. Systems with two to four electrodes are far more common; the studies reviewed in Section \ref{subsection:nature-of-eeg-signals} showed that, up until some saturation point, increasing the number of active channels invariably improves decoding accuracy. 

Not only would the presence of multiple channels very likely improve decoding performance in the existing decoding algorithms used, but it would also enable other multi-channel algorithms to be used. TRCA, as presented in Section \ref{sec:trca-c3}, is one such algorithm that requires multiple channels. This algorithm (and its extensions) has been shown to be very effective and would prove a worthy contender to the existing CCA-based algorithms used in this project. 

\subsection{Choice of decoding algorithms}

\subsection{Choice of development tools}
The tools used to develop this project, including the software, firmware and other technologies, were not only important during the development phase, but will continue to be after completion of this dissertation. The reason for this is that, as specified in the initial project scope and constraints mentioned in Chapter \ref{chapter:introduction}, a core objective for this project was to create a platform that can one day be used as an engagement and/or educational tool in the neurotechnology community. This factor strongly guided the choice of technologies in this project and ultimately, a system has been designed that uses exclusively open source tools. 

The choice of MicroPython for development with the ESP32 proved to be extremely prudent. MicroPython offers extremely simple yet elegant syntax and useful constructs that not only speed up development, but also make the process far more enjoyable. From an educational perspective, MicroPython is far more intuitive than C/C++ or their derivatives and shares the same syntax as Python; an extremely popular language for programming education. This also allowed for the use of the \texttt{ulab} module for linear algebra and other scientific computing that was absolutely indispensable to this project.

Finally, ease of development using the MicroPython ecosystem is undoubtedly the easiest and most convenient of all other options. In particular, a Jupyter Notebook can be used over a serial connection to a MicroPython-compatible board to offer immediate and interactive development, experimentation and debugging. Compared to more traditional embedded ecosystems that require any code updates to be flashed onto the target MCU and rerun, this interactive environment is remarkably efficient. All components of this system - including those responsible for data acquisition, filtering, computation and communication - were developed in an interactive notebook environment before being compiled into frozen bytecode on the device.


\section{Conclusion and Future Work}

\subsubsection{Future work}
While the system developed in this project performed remarkably well considering the constraints imposed on it, there are several areas for improvement. The following observations and suggestions are made for future extensions of this project. 
% Evaluate if objectives initially laid out were achieved
% - if not, why not
% - if they had been stated differently/more realistically, would the outcome have been different?
% - what would need to be done differently next time

\subsubsection{Multi-channel sensing}
As mentioned in the prior discussion, it is firmly believed that introducing additional EEG channels will significantly improve decoding performance. Furthermore, it is conceivable that the existing electronic hardware in this project could be adapted for multi-channel support by using a multi-channel analogue frontend (as is used on the OpenBCI Ganglion board, for example). The ESP32 SoC also has several available ADC inputs.

\subsubsection{Algorithm expansion}
While the algorithms selected in this project are some of the most popular and are close to the state-of-the-art at the time of writing, there are many other slight adjustments and extensions to these algorithms that warrant exploration. 

\subsubsection{Broader testing set}
As alluded to, the COVID-19 Pandemic made it difficult to gather data from different subjects during the course of this project. As a result, data was only collected on the author. This 

- mention how calibration averaging over many more people may make generalisation feasible



