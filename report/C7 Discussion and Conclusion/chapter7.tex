%% Chapter 7: Discussion and Conclusion
\chapter{Discussion and Conclusion}
\label{chapter:discussion-conclusion}

\graphicspath{ {report/Chapter7/assets/} } 

\section{Discussion}
% Lessons learnt

\subsection{Digital signal processing system}
The role of the DSP system in this project was paramount. Without appropriately processed signals, even the most sophisticated decoding algorithms would have been rendered useless. 

The time domain plot in Figure \ref{fig:sq-wave-time-c6} demonstrates effective sampling; the input square wave is captured with consistent frequency and amplitude. As is more visible in Figure \ref{fig:sq-wave-spectra-c6}, the estimated spectrum of the low-pass-filtered signal is almost exactly as desired; pass-band distortion is negligible and extremely steep roll-off is achieved in the transition band. Furthermore, stop-band attenuation is more than sufficient and, encouragingly, is very close to the 80dB design specification. Finally, after establishing effective sampling and filtering, the PSD estimate of the downsampled signal in Figure \ref{fig:sq-wave-ds-spectra-c6} verifies that the theoretical downsampling rate of $f_s'=\frac{1}{4}f_s = 64$Hz is able to maintain signal fidelity in the pass-band. The implication of this is that 4x fewer samples can be used to represent the original signal with negligible loss of information. This is invaluable for a system such as the one in this project that is severely constrained in memory and computational power.

In summary, the DSP system proved very effective and was verified to operate in very close agreement with its design parameters. Worth emphasising is the fact that all necessary DSP operations are able to be performed in firmware on the ESP32. Not only is this a prerequisite for the end-to-end on-device processing objective, but also, it makes for a more flexible and future-proof system. New filter weights or other DSP-related parameters can simply be flashed onto the device, or indeed, uploaded `over the air' through a network connection if needed.

\subsection{Decoding}
Broadly speaking, the decoding accuracy results in Chapter \ref{chapter:results} demonstrate that the system developed in this project is a very promising first prototype. Before any further analysis of these results, it should be emphasised that while impressive, they were computed using data collected exclusively from the author owing to COVID19-related circumstances. Notwithstanding, several trials were conducted under various conditions and a stringent evaluation procedure, as detailed in Section \ref{subsection:evaluating-results}, was employed in an attempt to provide more robust results. 

Trends in decoding accuracy with varying recording window size $T$ and number of calibration trials $p$ were largely similar across the GCCA and MsetCCA algorithms. Accordingly, these common trends are discussed below, followed by an analysis of their differences in Section \ref{subsection:algo-comparison-results}.

The trends in average classification accuracy across frequencies in Figure \ref{fig:gcca-acc-ns} and Figure \ref{fig:gcca-acc-nt} show a strong positive correlation to the length of the recording window $T$ used for measurement. This is a fairly obvious and expected result. It should be noted, however, that this increase in accuracy with $T$ was not exclusively monotonic for each stimulus frequency. For example, in Figure \ref{fig:gcca-acc-ns}, decoding accuracy of the 12Hz stimulus was higher for $T=1$s than $T=2s$. These infrequent fluctuations could be due to random noise or EMG or other signal artefacts present during one trial but not another. 

What is evident from both experiments with varying $T$ and $p$ is that the reliance on the number of calibration trials decreases with increasing $T$. For example, in Figure \ref{fig:gcca-acc-nt}, only $p=2$ calibration trials are required using $T=2$s windows to exceed the average accuracy for $p=4$ calibration trials when using $T=0.75$s windows. It can also be observed that the variance in decoding accuracy across stimulus frequencies for each test point\footnote{in this context, a test point is defined as an instance or set of results where the primary independent variable as measured on the x-axis is fixed. } decreases when more calibration trials are available. Interestingly, when only $p=1$ calibration trial is available, the results in Figure \ref{fig:gcca-acc-ns} and Figure \ref{fig:mcca-acc-ns} show an \textit{increase} in accuracy variance across frequencies with increasing $T$. 

\subsubsection{Comparison of accuracy across algorithms}
\label{subsection:algo-comparison-results}

\subsubsection{Online decoding}
One of the biggest challenges of this project was to develop a system capable of end-to-end, real-time on-device decoding. That is, an embedded system capable of performing all operations needed to acquire, process, decode and communicate signals without outsourcing any computation off the device. 

As documented in the system design in Chapter \ref{chapter:system-design} and demonstrated by the results in Chapter \ref{chapter:results}, one can conclude that this objective has been achieved. This is extremely significant as it means that a fully functional (albeit fairly basic) BCI device can be deployed in a small, self-contained package that is both completely mobile and extremely cost-effective. From a technical perspective, the fact that sophisticated decoding algorithms leveraging eigenvalue solvers and other high level linear algebra can be performed on a microcontroller - and with such ease of implementation - is quite remarkable.  

\subsubsection{Generalisation ability}
One of the most significant limitations of this system is the need for calibration during prior to each new recording session. A session, in this context, is defined as a distinct, continuous period in which the BCI headset was not removed or adjusted. Correspondingly, in order to separate sessions and truly test generalisation ability, it was required that the headset be removed between sessions. The results of the generalisation tests in Section \ref{subsection:generalisation-testing} show that using calibration data from one session was not helpful when used in preparation for inference in a different session. Referring to the convention explained in Section \ref{subsection:generalisation-testing}, Figure \ref{fig:gt-results-gcca} and Figure \ref{fig:gt-results-mcca} show very poor decoding performance for the case of no overlapping trials. This suggests that calibration and test data from two completely distinct sessions is not feasible. Interestingly, with $n>0$ overlapping trials, decoding accuracy was \textit{worse} than in the original decoding tests in Section \ref{subsection:decoding-acc-ns-effect} where calibration and test data sets were drawn from the same session (ultimately leaving less data available for calibration). In the case of overlapping trials, calibration was performed on both the whole first session \textit{and} a subset of the second session on which testing was performed. It is thus surprising that this overlapping case yielded poorer performance as the decoding algorithms had access to a superset of the calibration data available in the initial tests in Section \ref{subsection:decoding-acc-ns-effect}. 

These findings suggest that there is dramatic variance in recorded signals across sessions. This may be due to several factors:
\begin{enumerate}
    \item There was \textbf{no standardised positioning} of the BCI headband on the subject's head across sessions. Each time the headband was put on, an attempt was made to estimate consistent positioning but as the electrodes were positioned at the back of the head, this was difficult to do reliably without assistance (which was seldom available). Consequent deviations in the electrode positions across sessions would naturally produce noticeable variance in measured signals.
    \item Only dry electrodes could be used and \textbf{scalp-contact quality was inconsistent} at best. On some occasions, it was found that electrodes would be lodged in a more favourable orientation with better scalp contact than on others. This was caused by variations in hair partings and angle-of-location relative to the protruding inion at the back of the skull. 
    \item Albeit a differential measurement across two electrodes, only a \textbf{single EEG channel} was available. This poses more of a challenge for decoding algorithms to reject task-unrelated artefacts and random noise owing to having no other independent measurements available.  
\end{enumerate}

\subsection{Networking and communication}
The MQTT interface for publishing data to the AWS IoT Core service proved very effective. Although it was not formally tested, there were no recorded instances of packet loss. A limitation of having to use this protocol over public Internet is the slightly inconsistent latency experienced in publishing data. The distribution of time required to successfully publish decoded data to AWS (`publish time') in Figure \ref{subfig:timing-pub} demonstrates this phenomenon. The mean publish time is around 100ms with a standard deviation of 210ms which leaves most times below the 350ms mark. However, it was noticed that sporadically, times of over 2s would occur. This is likely due to fluctuations in network traffic - both on the Internet, the AWS service and most likely, within the personal network of the user. This is not a significant issue, however, as all data payloads are sent with timestamps; any readings received after a predetermined tolerance window could simply be discarded before resynchronising the system. 

\subsection{Limitations of the system and experimental procedures}
A summary of the limitations of the system alluded to above are as follows:
\begin{itemize}
    \item single channel measurement
    \item variable latency in the upstream link to AWS IoT core for publishing data
    \item 
\end{itemize}

Limitations in the experimental procedure followed include:
\begin{itemize}
    \item only being able to gather data from a single test subject
\end{itemize}


\subsection{Challenges encountered}
Discuss numerical challenges, precision issues, memory constraints
Discuss some of the auxiliary computational approaches used such as eigenvalue solvers etc


\subsection{Impact of constraints}
Besides the extremely limited budget, the constraint of having only a single channel likely proved to be the most significant in this project. The literature cited in Section \ref{section:existing-bci-tech-c2} suggests that very few, if any, BCI devices rely on a single measurement channel alone. Systems with two to four electrodes are far more common; the studies reviewed in Section \ref{subsection:nature-of-eeg-signals} showed that, up until some saturation point, increasing the number of active channels invariably improves decoding accuracy. 

Not only would the presence of multiple channels very likely improve decoding performance in the existing decoding algorithms used, but it would also enable other multi-channel algorithms to be used. TRCA, as presented in Section \ref{sec:trca-c3}, is one such algorithm that requires multiple channels. This algorithm (and its extensions) has been shown to be very effective and would prove a worthy contender to the existing CCA-based algorithms used in this project. 

\subsection{Choice of decoding algorithms}
TODO

\subsection{Choice of development tools}
The tools used to develop this project, including the software, firmware and other technologies, were not only important during the development phase, but will continue to be after completion of this dissertation. The reason for this is that, as specified in the initial project scope and constraints mentioned in Chapter \ref{chapter:introduction}, a core objective for this project was to create a platform that can one day be used as an engagement and/or educational tool in the neurotechnology community. This factor strongly guided the choice of technologies in this project and ultimately, a system has been designed that uses exclusively open source tools. 

The choice of MicroPython for development with the ESP32 proved to be extremely prudent. MicroPython offers extremely simple yet elegant syntax and useful constructs that not only speed up development, but also make the process far more enjoyable. From an educational perspective, MicroPython is far more intuitive than C/C++ or their derivatives and shares the same syntax as Python; an extremely popular language for programming education. This also allowed for the use of the \texttt{ulab} module for linear algebra and other scientific computing that was absolutely indispensable to this project.

Finally, ease of development using the MicroPython ecosystem is undoubtedly the easiest and most convenient of all other options. In particular, a Jupyter Notebook can be used over a serial connection to a MicroPython-compatible board to offer immediate and interactive development, experimentation and debugging. Compared to more traditional embedded ecosystems that require any code updates to be flashed onto the target MCU and rerun, this interactive environment is remarkably efficient. All components of this system - including those responsible for data acquisition, filtering, computation and communication - were developed in an interactive notebook environment before being compiled into frozen bytecode on the device.


\section{Conclusion and Future Work}

\subsubsection{Future work}
While the system developed in this project performed remarkably well considering the constraints imposed on it, there are several areas for improvement. The following observations and suggestions are made for future extensions of this project. 
% Evaluate if objectives initially laid out were achieved
% - if not, why not
% - if they had been stated differently/more realistically, would the outcome have been different?
% - what would need to be done differently next time

\subsubsection{Multi-channel sensing}
As mentioned in the prior discussion, it is firmly believed that introducing additional EEG channels will significantly improve decoding performance. Furthermore, it is conceivable that the existing electronic hardware in this project could be adapted for multi-channel support by using a multi-channel analogue frontend (as is used on the OpenBCI Ganglion board, for example). The ESP32 SoC also has several available ADC inputs.

\subsubsection{Algorithm expansion}
While the algorithms selected in this project are some of the most popular and are close to the state-of-the-art at the time of writing, there are many other slight adjustments and extensions to these algorithms that warrant exploration. 

\subsubsection{Broader testing set}
As alluded to, the COVID-19 Pandemic made it difficult to gather data from different subjects during the course of this project. As a result, data was only collected on the author. This 
- mention how calibration averaging over many more people may make generalisation feasible


% \subsubsection{More robust testing}
% Mention idea of switching squares around to test robustness

\subsubsection{Improving comfort of electrodes}
